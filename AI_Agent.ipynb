{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVEEckmVouLI"
      },
      "outputs": [],
      "source": [
        "### Install Required Dependencies (Run this cell first)\n",
        "!pip -q install chromadb openai langchain langchain-community langchain-openai tiktoken pymupdf\n",
        "!pip -q install google-generativeai langchain-community\n",
        "!pip install -q langchain-google-genai\n",
        "!pip -q install docx2txt unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UDc457l6pPMW"
      },
      "outputs": [],
      "source": [
        "### Import Necessary Libraries\n",
        "import os\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import AsyncChromiumLoader #directlyt import the data from an URL\n",
        "from langchain.document_transformers import Html2TextTransformer #converts html to text data\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouClyZoGpTuf",
        "outputId": "5648536f-00c5-4ef8-ad23-09ac32137dc3"
      },
      "outputs": [],
      "source": [
        "### Set API Key for Google Gemini\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqaOz2aepZEn",
        "outputId": "ef902d8a-2b06-4fd3-c8be-d0780bc112ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total pages loaded: 2\n"
          ]
        }
      ],
      "source": [
        "### Load and Process Documents\n",
        "# Load the PDF file\n",
        "# loader = PyMuPDFLoader(\"/content/FIlterign review.pdf\")\n",
        "# document = loader.load()\n",
        "\n",
        "document = []\n",
        "for filename in os.listdir(\"documents\"):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        loader = PyMuPDFLoader(os.path.join(\"documents\", filename))\n",
        "        document.extend(loader.load())\n",
        "\n",
        "print(f\"Total pages loaded: {len(document)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5enQi5gzp9Xe",
        "outputId": "2b1fe13a-c5c2-44ee-a653-e7cdf649c8a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split text into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "text_chunks = text_splitter.split_documents(document)\n",
        "\n",
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "S_lQQ6weqCqP"
      },
      "outputs": [],
      "source": [
        "### Set Up Vector Database (ChromaDB)\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "\n",
        "# Create or load a vector database\n",
        "persist_directory = 'db'\n",
        "vectordb = Chroma.from_documents(documents=text_chunks,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vcJr0OYKqE13"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YLJk-nlUu905"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "W2Nvmx7QvBSo"
      },
      "outputs": [],
      "source": [
        "### Function to Ask LLM First\n",
        "def ask_llm_first(query):\n",
        "    \"\"\"Ask the LLM first. If it confidently answers, return it. Otherwise, proceed to VectorDB.\"\"\"\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are an AI assistant. Answer only if you are confident about your response. \"\n",
        "        \"If you do not know the answer, simply respond with: 'I don't know.' \"\n",
        "        \"Do not guess or make assumptions.\\n\\n\"\n",
        "        f\"User Question: {query}\"\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(system_prompt)  # Query the LLM\n",
        "\n",
        "    # Extract text content from AIMessage object\n",
        "    if hasattr(response, \"content\"):\n",
        "        response_text = response.content  # Correct way to get the response text\n",
        "    else:\n",
        "        response_text = str(response)  # Fallback in case 'content' is missing\n",
        "\n",
        "    # Define phrases that indicate uncertainty\n",
        "    vague_responses = [\n",
        "        \"i don't know\", \"i am not sure\", \"i need more information\", \"please provide more details\",\n",
        "        \"i cannot determine\", \"i am unable to\", \"i do not have enough context\",\n",
        "        \"i need more context\", \"i do not have enough information\", \"i require additional details\"\n",
        "    ]\n",
        "\n",
        "     # Debugging: Print each phrase check\n",
        "    for phrase in vague_responses:\n",
        "        if phrase in response_text.lower():\n",
        "            print(f\"‚úÖ Match found: '{phrase}' is in LLM response!\")\n",
        "            print(\"‚ö†Ô∏è LLM is unsure. Fetching data from VectorDB...\")\n",
        "            return False  # Indicate retrieval is needed\n",
        "\n",
        "    print(\"ü§ñ LLM confidently answered without RAG.\")\n",
        "    return response_text  # Return LLM's answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uuxxGgZ-vEIY"
      },
      "outputs": [],
      "source": [
        "### Function to Perform RAG Retrieval\n",
        "def ask_with_rag(query):\n",
        "    \"\"\"Retrieve documents from VectorDB and generate an answer using LLM with context.\"\"\"\n",
        "    retrieved_docs = retriever.invoke(query)\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "    response = qa_chain.invoke({\"context\": retrieved_docs, \"query\": query})\n",
        "\n",
        "    print(\"\\nü§ñ AI Agent's Response (with RAG):\")\n",
        "    return response[\"result\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "19kMM9U79F2g"
      },
      "outputs": [],
      "source": [
        "def start_conversation():\n",
        "    \"\"\"Start an interactive chat session with the LLM and VectorDB.\"\"\"\n",
        "    print(\"\\nüí¨ Start chatting with the AI! (Type 'exit' to stop)\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input dynamically\n",
        "        user_query = input(\"üë§ You: \").strip()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Step 1: Ask LLM first\n",
        "        llm_response = ask_llm_first(user_query)\n",
        "\n",
        "        # Step 2: If LLM is unsure (False), use VectorDB retrieval\n",
        "        if llm_response is False:\n",
        "            final_answer = ask_with_rag(user_query)\n",
        "            print(f\"\\nü§ñ AI: {final_answer}\\n\")\n",
        "        else:\n",
        "            print(f\"\\nü§ñ AI: {llm_response}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6zPbHj39SLf",
        "outputId": "385d11c1-7341-4119-b50d-2aae8c235128"
      },
      "outputs": [],
      "source": [
        "start_conversation()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
